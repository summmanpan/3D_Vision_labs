{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: The geometry of two views and 3D reconstruction\n",
    "\n",
    "In this lab session, we are going to compute the 3D position of a pair of cameras and a set of keypoints.  For this, we will first compute correspondences between the two images.  Then, we will robustly compute the Fundamental matrix that encodes the geometry of the two views. From the Fundamental matrix and the camera calibration matrix -- that we learnt to estimate in the third lab session -- we will get the Essential matrix, which encodes the relative motion between the two cameras.  We will then compute the motion between the cameras and, finally, triangulate the matched keypoints to obtain their 3D position.\n",
    "\n",
    "The goals of this lab assignment are the following:\n",
    "\n",
    "- How to estimate the fundamental matrix that relates two images, corresponding to two different views of the same scene, given a set of correspondences between them. In particular, we will use the 8-point algorithm.\n",
    "\n",
    "- How to compute the relative pose of two calibrated cameras\n",
    "\n",
    "- How to triangulate point matches to reconstruct their 3D position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import cv2\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Estimation of the fundamental matrix**\n",
    "\n",
    "### **1.1 The 8-point algorithm**\n",
    "\n",
    "The first task is to create the function that estimates the fundamental matrix given a set of point correspondences between a pair of images. We provide the incomplete function `fundamental_matrix` that computes,\n",
    "with the normalised 8-point algorithm, an estimation of the Fundamental matrix\n",
    "from a set of tentative point correspondences.  \n",
    "\n",
    "**Q1.** Complete the function `fundamental_matrix` below.\n",
    "\n",
    "**Q2.** Why do we need to enforce the rank 2 constraint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(x):\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    x = x  / x[2,:]\n",
    "    \n",
    "    m, s = np.mean(x, 1), np.std(x)\n",
    "    s = np.sqrt(2)/s;\n",
    " \n",
    "    Tr = np.array([[s, 0, -s*m[0]], [0, s, -s*m[1]], [0, 0, 1]])\n",
    "\n",
    "\n",
    "    xt = Tr @ x\n",
    "        \n",
    "    return Tr, xt\n",
    "\n",
    "def fundamental_matrix(points1, points2):\n",
    "    \n",
    "    # Normalize points in both images\n",
    "    T1, points1n = Normalization(points1)\n",
    "    T2, points2n = Normalization(points2)\n",
    "\n",
    "    # complete ...\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below contains a toy example where we know the ground truth correspondences and ground-truth fundamental matrix. Use the code to test that the completed function is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two camera matrices for testing purposes\n",
    "P1 = np.zeros((3,4))\n",
    "P1[0,0]=P1[1,1]=P1[2,2]=1\n",
    "angle = 15\n",
    "theta = np.radians(angle)\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n",
    "t = np.array([[.3, .1, .2]])\n",
    "\n",
    "P2 = np.concatenate((R, t.T), axis=1)\n",
    "n = 8\n",
    "rand = np.random.uniform(0,1,n)\n",
    "rand = rand.reshape((1, n))\n",
    "rand2 = np.random.uniform(0,1,2*n)\n",
    "rand2 = rand2.reshape((2, n))\n",
    "ones = np.ones((1,n))\n",
    "X = np.concatenate((rand2, 3*rand, ones), axis=0)\n",
    "\n",
    "x1_test = P1 @ X\n",
    "x2_test = P2 @ X\n",
    "\n",
    "# Estimate fundamental matrix (you need to create this function)\n",
    "F_es = fundamental_matrix(x1_test, x2_test)\n",
    "\n",
    "# Ground truth fundamental matrix \n",
    "A = np.array([[0, -t[0,2], t[0,1]], [t[0,2], 0, -t[0,0]], [-t[0,1], t[0,0], 0]])\n",
    "F_gt = A @ R\n",
    "\n",
    "# Evaluation: these two matrices should be very similar\n",
    "F_gt = np.sign(F_gt[0,0])*F_gt / LA.norm(F_gt)\n",
    "F_es = np.sign(F_es[0,0])*F_es / LA.norm(F_es)\n",
    "print(F_gt)\n",
    "print(F_es)\n",
    "print(LA.norm(F_gt-F_es))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Robust estimation of the fundamental matrix**\n",
    "\n",
    "The next step is to robustly compute the Fundamental matrix from the point correspondences. For that we will use the function `ransac_fundamental_matrix` that you have to complete.\n",
    "\n",
    "The next code computes the image correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "img1 = cv2.imread('Data/manikin1.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/manikin2.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create(3000)\n",
    "\n",
    "# find the keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Show matches\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** A function `compute_inliers` is provided. How does it select the inliers? Explain also the error function used to find the inliers.\n",
    "\n",
    "\n",
    "**Q4.** Complete the function `ransac_fundamental_matrix` below. Which are the missing steps that needs to be added to the function? What is the difference with the RANSAC method used in the lab 3? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inliers(F, x1, x2, th):\n",
    "    \n",
    "    Fx1 = F @ x1\n",
    "    Ftx2 = F.T @ x2\n",
    "    \n",
    "    n = x1.shape[1]\n",
    "    x2tFx1 = np.zeros((1, n))\n",
    " \n",
    "    for i in range(n):\n",
    "        x2t = x2[:,i]\n",
    "        x2t = x2t.T\n",
    "        x2tFx1[0,i] = x2t @ F @ x1[:,i]\n",
    "    \n",
    "    # evaluate distances\n",
    "    den = Fx1[0,:]**2 + Fx1[1,:]**2 + Ftx2[0,:]**2 + Ftx2[1,:]**2\n",
    "    den = den.reshape((1, n))\n",
    "   \n",
    "    d = x2tFx1**2 / den\n",
    "    \n",
    "    inliers_indices = np.where(d[0,:] < th)\n",
    "    \n",
    "    return inliers_indices[0]\n",
    "\n",
    "\n",
    "\n",
    "def ransac_fundamental_matrix(points1, points2, th, max_it_0):\n",
    "    \n",
    "    Ncoords, Npts = points1.shape\n",
    "    \n",
    "    it = 0\n",
    "    best_inliers = np.empty(1)\n",
    "    max_it = max_it_0\n",
    "\n",
    "    while it < max_it:\n",
    "       \n",
    "        # complete ...\n",
    "            \n",
    "        # update estimate of iterations (the number of trials) to ensure we pick, with probability p, \n",
    "        # an initial data set with no outliers\n",
    "        fracinliers = inliers.shape[0]/Npts\n",
    "        pNoOutliers = 1 -  fracinliers**8\n",
    "        eps = sys.float_info.epsilon\n",
    "        pNoOutliers = max(eps, pNoOutliers)   # avoid division by -Inf\n",
    "        pNoOutliers = min(1-eps, pNoOutliers) # avoid division by 0\n",
    "        p = 0.99\n",
    "        max_it = math.log(1-p)/math.log(pNoOutliers)\n",
    "        if max_it > max_it_0:\n",
    "            max_it = max_it_0\n",
    "\n",
    "        it += 1\n",
    "        \n",
    "    \n",
    "    # compute H from all the inliers\n",
    "    F = fundamental_matrix(points1[:,best_inliers], points2[:,best_inliers])\n",
    "    \n",
    "    inliers = best_inliers\n",
    "    \n",
    "    print(it,inliers.shape[0])\n",
    "    \n",
    "    return F, inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust estimation of the fundamental matrix\n",
    "points1 = []\n",
    "points2 = []\n",
    "for m in matches:\n",
    "    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n",
    "    points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n",
    "    \n",
    "points1 = np.asarray(points1)\n",
    "points1 = points1.T\n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "\n",
    "F, indices_inlier_matches = ransac_fundamental_matrix(points1, points2, 2, 5000)\n",
    "inlier_matches = itemgetter(*indices_inlier_matches)(matches)\n",
    "\n",
    "img1 = cv2.imread('Data/manikin1.jpg')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.imread('Data/manikin2.jpg')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Epipolar lines**\n",
    "\n",
    "We will now visualize the epipolar lines associated to some points.\n",
    "\n",
    "**Q5.** Choose 3 inlier points in the first image and compute their corresponding epipolar lines (in homogeneous coordinates) in the second image. Do the same but now choosing points in the second image and showing their epipolar lines in the first image. Provide the commands you used for that. The commands for showing the lines on top of the image are already provided.\n",
    "\n",
    "\n",
    "**Q6.** Compute the epipoles from the Fundamental matrix. Compute the epipolar lines as the line that passes through the matching point and the epipole. Check that you obtain the same epipolar lines as before. Do the same with the images taken with your own camera during the lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = # ... epipolar lines in image 2 \n",
    "l1 = # ... epipolar lines in image 1\n",
    "\n",
    "# choose three random indices\n",
    "N = indices_inlier_matches.shape[0]\n",
    "indices = random.sample(range(1, N), 3)\n",
    "\n",
    "m1 = indices_inlier_matches[indices[0]]\n",
    "m2 = indices_inlier_matches[indices[1]]\n",
    "m3 = indices_inlier_matches[indices[2]]\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "from utils import line_draw, plot_img\n",
    "\n",
    "img_path = \"./Data/manikin1.jpg\"\n",
    "I = Image.open(img_path)\n",
    "I = I.rotate(-90, expand=True)\n",
    "size = I.size\n",
    "canv = ImageDraw.Draw(I)\n",
    "line_draw(l1[:,m1], canv, size)\n",
    "line_draw(l1[:,m2], canv, size)\n",
    "line_draw(l1[:,m3], canv, size)\n",
    "canv.ellipse((round(points1[0,m1]), round(points1[1,m1]), round(points1[0,m1])+15, round(points1[1,m1])+15), fill = 'yellow', outline ='yellow')\n",
    "canv.ellipse((round(points1[0,m2]), round(points1[1,m2]), round(points1[0,m2])+15, round(points1[1,m2])+15), fill = 'yellow', outline ='yellow')\n",
    "canv.ellipse((round(points1[0,m3]), round(points1[1,m3]), round(points1[0,m3])+15, round(points1[1,m3])+15), fill = 'yellow', outline ='yellow')\n",
    "plt.imshow(I)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "img_path = \"./Data/manikin2.jpg\"\n",
    "I2 = Image.open(img_path)\n",
    "I2 = I2.rotate(-90, expand=True)\n",
    "size = I2.size\n",
    "canv2 = ImageDraw.Draw(I2)\n",
    "line_draw(l2[:,m1], canv2, size)\n",
    "line_draw(l2[:,m2], canv2, size)\n",
    "line_draw(l2[:,m3], canv2, size)\n",
    "canv2.ellipse((round(points2[0,m1]), round(points2[1,m1]), round(points2[0,m1])+15, round(points2[1,m1])+15), fill = 'yellow', outline ='yellow')\n",
    "canv2.ellipse((round(points2[0,m2]), round(points2[1,m2]), round(points2[0,m2])+15, round(points2[1,m2])+15), fill = 'yellow', outline ='yellow')\n",
    "canv2.ellipse((round(points2[0,m3]), round(points2[1,m3]), round(points2[0,m3])+15, round(points2[1,m3])+15), fill = 'yellow', outline ='yellow')\n",
    "plt.imshow(I2)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Structure computation\n",
    "\n",
    "### **2.1 Triangulation**\n",
    "\n",
    "Before recovering the camera motion, we will first need to implement a function to triangulate matches.  This is a function that takes as input a match and the projection matrices of two cameras, and computes the 3D position of a match.\n",
    "\n",
    "We want then to find the 3D point $\\mathbf{X}$ such that its projection onto the images are $\\mathbf{x} = P \\mathbf{X}$ and $\\mathbf{x'} = P' \\mathbf{X}$ (being $P$ and $P'$ the projection matrices of each camera).\n",
    "\n",
    "**Q7.** Derive the system equations in the matrix form $A \\mathbf{X} = \\mathbf{0}$ that we need to solve in order to estimate the homogeneous coordinates of the 3D point $\\mathbf{X}$.\n",
    "\n",
    "**Q8.** Complete the code of the `triangulate`function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate(x1, x2, P1, P2, imsize):\n",
    "    \n",
    "    # only one point\n",
    "    if x1.ndim == 1:\n",
    "        x1 = np.array([x1]).T\n",
    "        x2 = np.array([x2]).T\n",
    "\n",
    "    # number of points\n",
    "    n = x1.shape[1]\n",
    "\n",
    "    # Normalization\n",
    "    x1 = x1/x1[2,:]\n",
    "    x2 = x2/x2[2,:]\n",
    "\n",
    "    nx = imsize[0];\n",
    "    ny = imsize[1];\n",
    "\n",
    "    H = [[2/nx,  0,     -1],\n",
    "        [0,      2/ny,  -1],\n",
    "        [0,      0,      1]]\n",
    "\n",
    "    x1_norm = H @ x1\n",
    "    x2_norm = H @ x2\n",
    "    P1_norm = H @ P1\n",
    "    P2_norm = H @ P2\n",
    "\n",
    "    X = # ... complete\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the following test code with a toy example to validate that the `triangulate`function is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two camera matrices for testing purposes\n",
    "P1 = np.zeros((3,4))\n",
    "P1[0,0]=P1[1,1]=P1[2,2]=1\n",
    "angle = 15\n",
    "theta = np.radians(angle)\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n",
    "t = np.array([[.3, .1, .2]])\n",
    "\n",
    "P2 = np.concatenate((R, t.T), axis=1)\n",
    "n = 8\n",
    "rand = np.random.uniform(0,1,n)\n",
    "rand = rand.reshape((1, n))\n",
    "rand2 = np.random.uniform(0,1,2*n)\n",
    "rand2 = rand2.reshape((2, n))\n",
    "ones = np.ones((1,n))\n",
    "X = np.concatenate((rand2, 3*rand, ones), axis=0)\n",
    "\n",
    "x1_test = P1 @ X\n",
    "x2_test = P2 @ X\n",
    "\n",
    "# Estimate the 3D points (you need to create this function)\n",
    "x_trian = np.zeros((4, n))\n",
    "x_trian = triangulate(x1_test, x2_test, P1, P2, ((2,2)));\n",
    "\n",
    "# Evaluation: compute the reprojection error\n",
    "x_eucl = x_trian / x_trian[3][np.newaxis]\n",
    "X_eucl = X / X[3][np.newaxis]\n",
    "\n",
    "diff = X_eucl - x_eucl\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Relative motion between the two cameras\n",
    "As seen in the theory class, the relative position between the cameras can be computed from the Essential matrix, $E$. In order to estimate $E$, we need the Fundamental matrix, $F$, that we have just computed and the camera calibration matrix, $K$, that we learnt how to estimate during the third session.\n",
    "\n",
    "**Q9.** Compute the Essential matrix from the Fundamental matrix and the camera calibration matrix in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K matrix estimated in lab 3 (you can use this one or the one you obtained)\n",
    "K = np.array([[3531.97, 9.59, 2304.33], [0, 3537.34, 1751.75], [0, 0, 1]]) \n",
    "\n",
    "E = # ... complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the first camera is at the origin, with no rotation, and that the second camera has a rotation and translation with respect to the first one.\n",
    "\n",
    "**Q10.** Write the camera projection matrix $P$ for the first camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = # ... complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rotation and translation of the second camera can be computed from the SVD decomposition of $E$. There are four possible solutions.\n",
    "\n",
    "**Q11.** Complete the code below to compute the four candidate solutions for the second camera projection matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,D,VT = LA.svd(E)\n",
    "\n",
    "# The SVD of E has several ambiguities.  In particular, U and V may be\n",
    "# improper rotations in which case we need to change their sign.\n",
    "if LA.det(U) < 0:\n",
    "    U = -U\n",
    "    \n",
    "if LA.det(VT) < 0:\n",
    "    VT = -VT\n",
    "\n",
    "# four camera projection matrices for the second camera\n",
    "Pc2 = np.empty(shape=(4,3,4))\n",
    "Pc2[0] = # ... complete\n",
    "Pc2[1] = # ... complete\n",
    "Pc2[2] = # ... complete\n",
    "Pc2[3] = # ... complete\n",
    "\n",
    "from utils import plot_camera\n",
    "import plotly.graph_objects as go    \n",
    "\n",
    "ny, nx = I.size\n",
    "\n",
    "fig = go.Figure()\n",
    "plot_camera(P1, nx, ny, fig, \"ref. camera\")\n",
    "for i in range(4):\n",
    "    plot_camera(Pc2[i], nx, ny, fig, \"camera2_{0}\".format(i))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** How can we choose the right solution from the four candidates? The code to do it is provided below, explain how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = points1[:,indices_inlier_matches]\n",
    "x2 = points2[:,indices_inlier_matches]\n",
    "\n",
    "for P2i in Pc2:\n",
    "\n",
    "    Xi = triangulate(x1[:,0], x2[:,0], P1, P2i, [nx, ny])\n",
    "    Xi = Xi / Xi[3][np.newaxis]\n",
    "    \n",
    "    x1est = P1 @ Xi\n",
    "    x2est = P2i @ Xi\n",
    "     \n",
    "    if (x1est[2] > 0) and (x2est[2] > 0):\n",
    "        P2 = P2i\n",
    "        break\n",
    "\n",
    "fig = go.Figure()\n",
    "plot_camera(P1, nx, ny, fig, \"camera 1\")\n",
    "plot_camera(P2, nx, ny, fig, \"camera 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Reconstruction from two views**\n",
    "\n",
    "Once the proper solution for the relative motion between cameras is chosen, we can triangulate all the matches to get a sparse point cloud. \n",
    "\n",
    "**Q13.** Complete the code to triangulate all matches.\n",
    "\n",
    "Use the provided code to plot the reconstructed points. If everything went fine, you should recognize the sparse points corresponding to the different parts of the manikin body at their corresponding 3D positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triangulate all matches \n",
    "X = # complete ...\n",
    "\n",
    "# Render the 3D point cloud\n",
    "fig = go.Figure()\n",
    "plot_camera(P1, nx, ny, fig, \"camera 1\")\n",
    "plot_camera(P2, nx, ny, fig, \"camera 2\")\n",
    "x_img = np.transpose(x1[:2]).astype(int)\n",
    "img1 = cv2.imread('Data/manikin1.jpg')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "rgb_vals = (img1[x_img[:,1], x_img[:,0]])\n",
    "rgb_vals = [f'rgb{tuple(x)}' for x in rgb_vals]\n",
    "\n",
    "point_color = [(255, 0, 0),(0,255,0)]\n",
    "fig.add_trace(go.Scatter3d(x=X[0,:], y=X[2,:], z=-X[1,:], mode='markers',marker=dict(size=2,color=rgb_vals)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to compute the reprojection error of the reconstructed points.  This is the distance between the detected keypoints, `x1` and `x2`, and the projection of the reconstructed points.\n",
    "\n",
    "**Q14.** Complete the code to compute the reprojection error of each match. Plot the histogram of the errors with the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojection_errors(x1,x2,P1,P2,X):\n",
    "    \n",
    "    # ... complete\n",
    "\n",
    "\n",
    "err = reprojection_errors(x1,x2,P1,P2,X)\n",
    "plt.hist(err,bins='auto')\n",
    "err_mean = np.mean(err)\n",
    "print(f'Mean reprojection error: {err_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15.** Repeat all the process with your own images taken with your\n",
    "camera. Try reconstructions for different pairs of images and comment the results\n",
    "obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
