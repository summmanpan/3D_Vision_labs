{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2. Robust homography estimation: building image mosaics\n",
    "\n",
    "In this practical session, we will create panoramic images by stitching multiple images of a scene taken from the same position, but pointing the camera to slightly different directions.  These images are related by homographies that we will need to compute.  The process will have the following steps:\n",
    "\n",
    "- The first step will be to compute keypoints in the images that we can use to find correspondences.  We will use SIFT or ORB keypoints and features.\n",
    "- Next we will compute correspondences between sets of keypoint features. We will use existing code for these first two steps.\n",
    "- From the correspondences we will compute the homography that maps one image into the another. Since some correspondences may be erroneous, the computation will have to be robust to outliers. We will use the \"RANdom SAmple Consensus\" (RANSAC) method that you will have to complete.\n",
    "- Finally, with the homographies computed, we will map all the images into a common canvas by using a variant of the  `apply_H` function from the last assignment.\n",
    "\n",
    "Finally, you will have to answer the questions and complete the provided code when necessary as required. Some questions are rather theoretical and eventually imply some calculations while others are practical questions. **You must deliver the completed (and executed) ipynb file and a pdf file with the answers to the questions (included the parts of the code that need to be added).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install plotly\n",
    "# tenemos esto comentado en el utils,!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from utils import apply_H_fixed_image_size, Normalization, DLT_homography\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python==4.4.0.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Compute image correspondences**\n",
    "\n",
    "The first step is to read the images and to compute their keypoints. The images are RGB. We have to convert them to gray scale with values in order to compute the keypoints on them. Then, we compute the keypoints and descriptors of every image. For the parts where the content of the two images coincide, you can visually check that many of the detected points are detected in both images. We want to find these _correspondences_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method(type_num):\n",
    "    switcher = {\n",
    "        0: \"ORB\",\n",
    "        1: \"SIFT\",\n",
    "    }\n",
    " \n",
    "    # get() method of dictionary data type returns\n",
    "    # value of passed argument if it is present\n",
    "    # in dictionary otherwise second argument will\n",
    "    # be assigned as default value of passed argument\n",
    "    return switcher.get(type_num, \"nothing\")\n",
    "## ESTARÍA GUAY PONER UN SWITCH Y LLAMAR A UNA FUNCIÓN U OTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img1 = cv2.imread('Data/Llanes/llanes_a.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/Llanes/llanes_b.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img3 = cv2.imread('Data/Llanes/llanes_c.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create(3000)\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title('Image 1')\n",
    "ax2.set_title('Image 2')\n",
    "ax1.imshow(img1b); ax2.imshow(img2b)\n",
    "plt.show()\n",
    "\n",
    "# OBR method\n",
    "flag_method = get_method(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Are the keypoints uniformly distributed over the image? Where are there more keypoints? Why?\n",
    "\n",
    "\n",
    "To match the keypoints between two images, we need to assign to each keypoint in the first image the one that has the most similar descriptor in the second image. \n",
    "\n",
    "Execute the following code to find image correspondences using ORB [1] (you may also use SIFT [2], SURF [3], etc, an example of the use of SIFT is given in another code cell below).\n",
    "\n",
    "[1] Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary R. Bradski. ORB: An efficient alternative to SIFT or SURF. ICCV, 2564-2571, 2011.\n",
    "\n",
    "[2] David Lowe. Object recognition from local scale-invariant features. ICCV, 1150-1157, 1999.\n",
    "\n",
    "[3] Herbert Bay, Tinne Tuytelaars, Luc Van Gool. Surf: Speeded up robust features. ECCV, 404-417, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <font size=\"3\"> **Answer:** </mark>\n",
    "\n",
    "en los puntos donde hay mas cambios de gradientes de la misma imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **ORB:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoint matching\n",
    "# OBR method\n",
    "bf = cv2.BFMatcher()\n",
    "# def compute_orb(flag_method, img1, img2, des1, des2, kp1, kp2, bf):\n",
    "#     if flag_method != \"ORB\":\n",
    "#         return\n",
    "    \n",
    "    \n",
    "matches_12 = bf.knnMatch(des1,des2,k=2)\n",
    "# Apply ratio test\n",
    "good_matches_12 = []\n",
    "for m,n in matches_12:\n",
    "\n",
    "    if m.distance < 0.85*n.distance:\n",
    "        good_matches_12.append([m])\n",
    "\n",
    "# Show \"good\" matches \n",
    "img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "    \n",
    "#     return good_matches_12\n",
    "    \n",
    "# good_matches_12 = compute_orb(flag_method, img1, img2, des1, des2, kp1, kp2, bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to find SIFT correspondences you may use the code below. \n",
    "\n",
    "NOTE: SIFT is not available in certain OpenCV versions. A version that worked for us was opencv-contrib 4.4.0.46, you can install it with the following command: `pip install opencv-contrib-python==4.4.0.46`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> **SIFT:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "# def compute_sift(flag_method, img1, img2, des1, des2, kp1, kp2, bf ):\n",
    "#     if flag_method != \"SIFT\":\n",
    "#         return\n",
    "\n",
    "# #     bf = cv2.BFMatcher()\n",
    "\n",
    "if flag_method != \"ORB\":\n",
    "\n",
    "    sift = cv2.SIFT_create(3000)\n",
    "\n",
    "    # find the keypoints and descriptors\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "    # draw only keypoints location,not size and orientation\n",
    "    img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img1b), plt.show()\n",
    "\n",
    "    # Keypoint matching\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches_12 = bf.match(des1,des2)\n",
    "\n",
    "    # Show matches\n",
    "    img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n",
    "#     return matches_12\n",
    "\n",
    "# matches_12 = compute_sift(flag_method, img1, img2, des1, des2, kp1, kp2, bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.**\n",
    "Compute and visualize the matchings between image 2 and 3. Write the commands you used for that.\n",
    "\n",
    "**Q3.**\n",
    "Are all the matchings correct? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <font size=\"3\"> **Answer:** </mark>\n",
    "    \n",
    "They are not the same, since they have a slighly change of view of the imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <font size=\"3\"> **Between image 2 and 3:** </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "\n",
    "orb = cv2.ORB_create(3000)\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "kp3, des3 = orb.detectAndCompute(img3,None)\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "img3b = cv2.drawKeypoints(img3, kp3, None, color=(0,255,0), flags=0)\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title('Image 2')\n",
    "ax2.set_title('Image 3')\n",
    "ax1.imshow(img2b); ax2.imshow(img3b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **ORB:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoint matching \n",
    "bf = cv2.BFMatcher()\n",
    "matches_23 = bf.knnMatch(des2,des3,k=2)\n",
    "# Apply ratio test\n",
    "good_matches_23 = []\n",
    "for m,n in matches_23:\n",
    "    \n",
    "    if m.distance < 0.85*n.distance:\n",
    "        good_matches_23.append([m])\n",
    "\n",
    "# Show \"good\" matches \n",
    "img_23 = cv2.drawMatchesKnn(img2,kp2,img3,kp3,good_matches_23,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_23)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **SIFT:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create(3000)\n",
    "\n",
    "# find the keypoints and descriptors\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "kp3, des3 = sift.detectAndCompute(img3,None)\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "plt.imshow(img2b), plt.show()\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches_23 = bf.match(des2,des3)\n",
    "\n",
    "# Show matches\n",
    "img_23 = cv2.drawMatches(img2,kp2,img3,kp3,matches_23,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_23)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Compute the homography (robust DLT algorithm) between image pairs**\n",
    "\n",
    "We want now to compute the homography that relates each pair of images. From  the last assignment, we have a function called `DLT_homography` that computes a homography given a set of correspondences. Unfortunately, this only works when all of the correspondences are correct, which is not the case in most practical applictions as the current one. This time, we will need to use the RANSAC method in order to find the correct correspondences and discard the others.\n",
    "\n",
    "For that we use the functions `Ransac_DLT_homography` and `Inliers` that you have \n",
    "to complete below (this is part of questions Q5 and Q6 below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inliers(H, points1, points2, th):\n",
    "   \n",
    "    # Check that H is invertible\n",
    "    if abs(math.log(np.linalg.cond(H))) > 15: \n",
    "        idx = np.empty(1)\n",
    "        return idx\n",
    "    \n",
    "    inliers_indices = []\n",
    "    # to normalize in case it is not\n",
    "    points1/=points1[2,:]\n",
    "    points2/=points2[2,:]\n",
    "    # complete this code .......\n",
    "    for j in range(points1.shape[1]):\n",
    "        x_prima_j = points2[:,j]\n",
    "        x_j = points1[:,j]\n",
    "        \n",
    "        dist1 = x_prima_j - np.dot(H, x_j)\n",
    "        dist2 = np.dot(np.linalg.inv(H),x_prima_j)-x_j\n",
    "        dist = np.linalg.norm(dist1) + np.linalg.norm(dist2)\n",
    "        \n",
    "        if dist<th:\n",
    "            inliers_indices.append(j)\n",
    "    \n",
    "    \n",
    "    return np.asarray(inliers_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ransac_DLT_homography(points1, points2, th, max_it):\n",
    "    \n",
    "    Ncoords, Npts = points1.shape\n",
    "    \n",
    "    it = 0\n",
    "    best_inliers = np.empty(1)\n",
    "\n",
    "    while it < max_it:\n",
    "        # choose only 4 samples\n",
    "        L = 4 # number of samples to choose randomly\n",
    "        indices = random.sample(range(1, Npts), L)\n",
    "        # compute the homography for only those 4 samples \n",
    "        H = DLT_homography(points1[:,indices], points2[:,indices])\n",
    "        # taking the computed H, check how much it relates to all points in the dataset\n",
    "        # check the number of inliers that we obtain with that homograpgy\n",
    "        inliers = Inliers(H, points1, points2, th)\n",
    "        \n",
    "        # test if it is the best model so far\n",
    "        # save only the best inliers of all iterations, namely the one with more inliers\n",
    "        if inliers.shape[0] > best_inliers.shape[0]:\n",
    "            best_inliers = inliers\n",
    "            \n",
    "        # iterate again in case in the next iteration we obtain something better\n",
    "        it += 1\n",
    "    \n",
    "    # compute H from all the inliers\n",
    "    H = DLT_homography(points1[:,best_inliers], points2[:,best_inliers])\n",
    "    inliers = best_inliers\n",
    "    \n",
    "    return H, inliers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows to robustly estimate the homography that relates images 1 and 2. Examine the code and answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ORB:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Homography between images 1 and 2\n",
    "# if obr\n",
    "if (flag_method == \"ORB\"):\n",
    "    points1 = []\n",
    "    points2 = []\n",
    "    for m in good_matches_12: # matches_12 instead, if you use SIFT\n",
    "        points1.append([kp1[m[0].queryIdx].pt[0], kp1[m[0].queryIdx].pt[1], 1]) # m.queryIdx instead, if you use SIFT\n",
    "        points2.append([kp2[m[0].trainIdx].pt[0], kp2[m[0].trainIdx].pt[1], 1]) # m.trainIdx instead, if you use SIFT\n",
    "\n",
    "    points1 = np.asarray(points1)\n",
    "    points1 = points1.T\n",
    "    points2 = np.asarray(points2)\n",
    "    points2 = points2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIFT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Homography between images 1 and 2 SIFTTTT\n",
    "if (flag_method == \"SIFT\"):\n",
    "    points1 = []\n",
    "    points2 = []\n",
    "    for m in matches_12: # matches_12 instead, if you use SIFT\n",
    "        points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1]) # m.queryIdx instead, if you use SIFT\n",
    "        points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1]) # m.trainIdx instead, if you use SIFT\n",
    "\n",
    "    points1 = np.asarray(points1)\n",
    "    points1 = points1.T\n",
    "    points2 = np.asarray(points2)\n",
    "    points2 = points2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OBR\n",
    "if flag_method == \"ORB\":\n",
    "    th = 10\n",
    "    H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1, points2, th, 1000)\n",
    "    inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(good_matches_12) # matches_12 instead, if you use SIFT\n",
    "\n",
    "    # drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "    img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIFT\n",
    "if flag_method == \"SIFT\":\n",
    "    th = 10\n",
    "    H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1, points2, th, 1000)\n",
    "    inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(matches_12) # matches_12 instead, if you use SIFT\n",
    "\n",
    "    print('indices: ', len(indices_inlier_matches_12))\n",
    "    print('points: ', len(inlier_matches_12))\n",
    "    \n",
    "    # drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "    img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** What are the variables `points1` and `points2`?\n",
    "\n",
    "Complete the `Ransac_DLT_homography` function by answering the following questions:\n",
    "\n",
    "**Q5.** Set the number of samples to choose randomly and that will define the model to test in each trial. (second input parameter of function `random.sample`).\n",
    "\n",
    "**Q6.** Complete the function `Inliers` by computing the geometric error of the correspondences given the homography.\n",
    "\n",
    "**Q7.** What is the input parameter `th` when calling the `Ransac_DLT_homography` function? Choose a good value for this parameter and justify your answer.\n",
    "\n",
    "**Q8.** Create a new function `Ransac_DLT_homography_adaptive_loop` which is based on the function `Ransac_DLT_homography` and automatically adapts the number of trials to ensure we pick, with a probability $p=0.99$ an initial data set with no outliers.\n",
    "\n",
    "**Q9.** Compare experimentally the two versions of the RANSAC algorithm (`Ransac_DLT_homography` and `Ransac_DLT_homography_adaptive_loop`) in terms of the number of iterations. Which version is better and why?\n",
    "\n",
    "**Q10.** Compute the homography that relates images 2 and 3. Write the commands you used for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "- Q4: son los maches entre los key points de la imagen 1 e imagen 2\n",
    "- Q5: aunque se podrían escoger más samples, el mínimo que necesitamos para poder calcular la homografía son cuatro samples.\n",
    "- Q7: es la distancia máxima (entre los puntos y sus proyecciones) para considerar que un match es descriptivo. HAY QUE HACER PRUEBA Y ERROR PARA ESCOGER EL THRESHOLD!!!!!!!!!!! SE PUEDE HACER UN BUCLE O A LO CUTRE\n",
    "- Q9: la diferencia entre ambos métodos, es que al adaptar el numero de iteraciones en el segundo ganamos mucha eficiencia. Experimentalemente: observamos que con 1000 iteraciones en el primero, obtenemos aproximadamente el mismo numero de inliers (439) que en el segundo (aprox. 400-500) pero con muchas menos iteraciones, (140), con lo que ganamos mucho tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to complete here ...\n",
    "def Ransac_DLT_homography_adaptive_loop(points1, points2, th):\n",
    "    \n",
    "    Ncoords, Npts = points1.shape\n",
    "    \n",
    "    it = 0\n",
    "    best_inliers = np.empty(1)\n",
    "    \n",
    "    N = 10000\n",
    "    sample_count = 0\n",
    "    s = 4\n",
    "    p = 0.99\n",
    "    max_it = 1000\n",
    "    while N > sample_count and sample_count < 1000:\n",
    "        \n",
    "        L = 4 # number of samples to choose randomly\n",
    "        indices = random.sample(range(1, Npts), L)\n",
    "        H = DLT_homography(points1[:,indices], points2[:,indices])\n",
    "        inliers = Inliers(H, points1, points2, th)\n",
    "        \n",
    "        # test if it is the best model so far\n",
    "        if inliers.shape[0] > best_inliers.shape[0]:\n",
    "            best_inliers = inliers\n",
    "        \n",
    "        num_best_inliers = len(best_inliers)\n",
    "        w = num_best_inliers / points1.shape[1]\n",
    "        N = np.log(1-p)/np.log(1-(w**s))\n",
    "        print(sample_count)\n",
    "        \n",
    "#         print('N: ', N, '\\n')\n",
    "        \n",
    "        sample_count += 1\n",
    "            \n",
    "        \n",
    "    # compute H from all the inliers\n",
    "    H = DLT_homography(points1[:,best_inliers], points2[:,best_inliers])\n",
    "    inliers = best_inliers\n",
    "    \n",
    "    return H, inliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANVIAR A LA NUEVA FUNCIÓN\n",
    "# # OBR\n",
    "if flag_method == \"ORB\":\n",
    "    th = 10\n",
    "    H_12, indices_inlier_matches_12 = Ransac_DLT_homography_adaptive_loop(points1, points2, th)\n",
    "    inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(good_matches_12) # matches_12 instead, if you use SIFT\n",
    "\n",
    "    # drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "    img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "if flag_method == \"SIFT\":\n",
    "    th = 10\n",
    "    H_12, indices_inlier_matches_12 = Ransac_DLT_homography_adaptive_loop(points1, points2, th)\n",
    "    inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(matches_12) # matches_12 instead, if you use SIFT\n",
    "\n",
    "    \n",
    "    print('indices: ', len(indices_inlier_matches_12))\n",
    "    print('points: ', len(inlier_matches_12))\n",
    "    \n",
    "    # drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "    img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Homography between images 2 and 3\n",
    "# if obr\n",
    "if (flag_method == \"ORB\"):\n",
    "    points2 = []\n",
    "    points3 = []\n",
    "    for m in good_matches_23: # matches_23 instead, if you use SIFT\n",
    "        points3.append([kp2[m[0].queryIdx].pt[0], kp2[m[0].queryIdx].pt[1], 1]) # m.queryIdx instead, if you use SIFT\n",
    "        points3.append([kp3[m[0].trainIdx].pt[0], kp3[m[0].trainIdx].pt[1], 1]) # m.trainIdx instead, if you use SIFT\n",
    "\n",
    "    points2 = np.asarray(points2)\n",
    "    points2 = points2.T\n",
    "    points3 = np.asarray(points3)\n",
    "    points3 = points3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Homography between images 2 and 3 SIFTTTT\n",
    "if (flag_method == \"SIFT\"):\n",
    "    points2 = []\n",
    "    points3 = []\n",
    "    for m in matches_23: # matches_23 instead, if you use SIFT\n",
    "        points2.append([kp2[m.queryIdx].pt[0], kp2[m.queryIdx].pt[1], 1]) # m.queryIdx instead, if you use SIFT\n",
    "        points3.append([kp3[m.trainIdx].pt[0], kp3[m.trainIdx].pt[1], 1]) # m.trainIdx instead, if you use SIFT\n",
    "\n",
    "    points2 = np.asarray(points2)\n",
    "    points2 = points2.T\n",
    "    points3 = np.asarray(points3)\n",
    "    points3 = points3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before but with img 2,3\n",
    "\n",
    "# FALTA 10\n",
    "\n",
    "# # OBR\n",
    "if flag_method == \"ORB\":\n",
    "    th = 10\n",
    "    H_23, indices_inlier_matches_23 = Ransac_DLT_homography_adaptive_loop(points2, points3, th)\n",
    "    inlier_matches_23 = itemgetter(*indices_inlier_matches_23)(good_matches_23) # matches_23 instead, if you use SIFT\n",
    "\n",
    "    # drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "    img_23 = cv2.drawMatchesKnn(img2,kp2,img3,kp3,inlier_matches_23,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_23)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "if flag_method == \"SIFT\":\n",
    "    th = 10\n",
    "    H_23, indices_inlier_matches_23 = Ransac_DLT_homography_adaptive_loop(points2, points3, th)\n",
    "#     H_23, indices_inlier_matches_23 = Ransac_DLT_homography(points2, points3, th,1000)\n",
    "    inlier_matches_23 = itemgetter(*indices_inlier_matches_23)(matches_23) # matches_23 instead, if you use SIFT\n",
    "\n",
    "    \n",
    "    print('indices: ', len(indices_inlier_matches_23))\n",
    "    print('points: ', len(inlier_matches_23))\n",
    "    \n",
    "    # drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "    img_23 = cv2.drawMatches(img2,kp2,img3,kp3,inlier_matches_23,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_23)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Build the mosaic**\n",
    "\n",
    "At this point we have all the ingredients to built the image mosaic. For transforming an image with a specified homography we use a modification of the function `apply_H` used in the previous practical session. The modified function `apply_H_fixed_image_size` transforms the input image according to the input homography and writes it in an output image of size corresponding to the input vector of desired corner coordinates.\n",
    "\n",
    "Examine and complete the code below when necessary.\n",
    "\n",
    "**Q11.** Which homography we use for transforming the image 2? Why?\n",
    "\n",
    "**Q12.** How are the two images combined in the mosaic image? Why?\n",
    "\n",
    "**Q13.** Complete the mosaic including the third image. Write the commands you used for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer>\n",
    "\n",
    "- Q11:\n",
    "- Q12:\n",
    "- Q13:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = [-400, 1200, -100, 650]\n",
    "\n",
    "#print(img1.shape)\n",
    "img1c = cv2.imread('Data/Llanes/llanes_a.jpg',cv2.IMREAD_COLOR)\n",
    "img2c = cv2.imread('Data/Llanes/llanes_b.jpg',cv2.IMREAD_COLOR)\n",
    "img3c = cv2.imread('Data/Llanes/llanes_c.jpg',cv2.IMREAD_COLOR)\n",
    "img1c = cv2.cvtColor(img1c, cv2.COLOR_BGR2RGB)\n",
    "img2c = cv2.cvtColor(img2c, cv2.COLOR_BGR2RGB)\n",
    "img3c = cv2.cvtColor(img3c, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# mosaic 1-2\n",
    "img1c_w = apply_H_fixed_image_size(img1c, H_12, corners)\n",
    "img2c_w = apply_H_fixed_image_size(img2c, np.identity(3), corners)\n",
    "img_mosaic_12 = np.maximum(img1c_w,img2c_w)\n",
    "plt.imshow(img_mosaic_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "# mosaic 2-3\n",
    "img2c_w = apply_H_fixed_image_size(img2c, H_23, corners)\n",
    "img3c_w = apply_H_fixed_image_size(img3c,np.identity(3) , corners)\n",
    "img_mosaic_23 = np.maximum(img2c_w,img3c_w)\n",
    "plt.imshow(img_mosaic_23)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "# mosaic 1-2-3\n",
    "\n",
    "# ... complete ...\n",
    "img1c_w_p = apply_H_fixed_image_size(img1c,H_12, corners)\n",
    "img2c_w_p = apply_H_fixed_image_size(img2c,np.identity(3), corners) \n",
    "img3c_w_p = apply_H_fixed_image_size(img3c,np.linalg.inv(H_23), corners)\n",
    "\n",
    "img_mosaic_12_p = np.maximum(img1c_w_p, img2c_w_p)\n",
    "img_mosaic_23_p = np.maximum(img2c_w_p, img3c_w_p)\n",
    "img_mosaic_123 = np.maximum(img_mosaic_12_p, img_mosaic_23_p)\n",
    "\n",
    "plt.imshow(img_mosaic_123)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14.** **(Optional)** Create a function that takes as input the path to the folder where the images to construct the panorama are located. The function should work independently of the number of images that are located in the folder (assume that the images are given in the correct order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img3c = cv2.imread('Data/Arboles/Arboles1.png',cv2.IMREAD_COLOR)\n",
    "img1c = cv2.cvtColor(img3c, cv2.COLOR_BGR2RGB)\n",
    "           \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title('Image 1')\n",
    "ax2.set_title('Image 2')\n",
    "ax1.imshow(img3c); ax2.imshow(img1c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from utils import apply_H_fixed_image_size, Normalization, DLT_homography\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panorama(path_images):\n",
    "    # INPUTS:\n",
    "        #path_images: path to the folder that contain the images\n",
    "    # OTPUT: \n",
    "        # P: panoarama image \n",
    "        \n",
    "    \n",
    "    # READ ALL THE IMAGES OF THE INPUT PATH  \n",
    "    folders = os.listdir(path_images)[1:]\n",
    "    for folder in folders:\n",
    "#         if folder != 'Llanes':\n",
    "#             continue\n",
    "#         images = os.listdir(path_images, '/', folder)\n",
    "        images = os.listdir(path_images + '/'+ folder)\n",
    "        if '.DS_Store' in images:\n",
    "            images.remove('.DS_Store')\n",
    "        \n",
    "        img_list_gray = []\n",
    "        img_list_rgb = []\n",
    "        for image in images:\n",
    "            full_path = path_images + '/' + folder + '/' + image\n",
    "            print(full_path)\n",
    "            img_list_gray.append(cv2.imread(full_path,cv2.IMREAD_GRAYSCALE))\n",
    "            img_list_rgb.append(cv2.imread(full_path,cv2.IMREAD_COLOR))\n",
    "#             img_list_rgb.append(cv2.cvtColor( cv2.imread(image,cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        img1 = img_list_gray[0]\n",
    "        img2 = img_list_gray[1]\n",
    "#         img1c = img_list_rgb[0]\n",
    "#         img2c = img_list_rgb[1]\n",
    "        \n",
    "        img1c = cv2.cvtColor(img_list_rgb[0], cv2.COLOR_BGR2RGB)\n",
    "        img2c = cv2.cvtColor(img_list_rgb[1], cv2.COLOR_BGR2RGB)\n",
    "        # COMPUTE THE IMAGE KEYPOINTS OF THE IMAGES\n",
    "\n",
    "        # ORB\n",
    "        orb = cv2.ORB_create(3000)\n",
    "        # find the keypoints and descriptors with ORB\n",
    "        kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "        kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "        # draw only keypoints location,not size and orientation\n",
    "        img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "        img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches_12 = bf.knnMatch(des1,des2,k=2)\n",
    "        # Apply ratio test\n",
    "        good_matches_12 = []\n",
    "        for m,n in matches_12:\n",
    "\n",
    "            if m.distance < 0.85*n.distance:\n",
    "                good_matches_12.append([m])\n",
    "        # SIFT\n",
    "\n",
    "        sift = cv2.SIFT_create(3000)\n",
    "\n",
    "        # find the keypoints and descriptors\n",
    "        kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "        # MATCH THE KEYPOINTS\n",
    "        print(\"MATCH THE KEYPOINTS DONETE\")\n",
    "        \n",
    "        #PLOT of KEYPOINTS draw only keypoints location,not size and orientation\n",
    "#         img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "#         plt.imshow(img1b), plt.show()\n",
    "        \n",
    "        # Keypoint matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "        matches_12 = bf.match(des1,des2)\n",
    "        \n",
    "        \n",
    "         #  PLOT 2Show matches\n",
    "#         img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#         plt.imshow(img_12)\n",
    "#         fig = matplotlib.pyplot.gcf()\n",
    "#         fig.set_size_inches(18.5, 10.5)\n",
    "#         plt.show()\n",
    "    \n",
    "        #COMPUTE THE HOMOGRAPIES\n",
    "        print(\"COMPUTE THE HOMOGRAPIES\")\n",
    "        points1 = []\n",
    "        points2 = []\n",
    "        for m in matches_12: # matches_12 instead, if you use SIFT\n",
    "            points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1]) # m.queryIdx instead, if you use SIFT\n",
    "            points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1]) # m.trainIdx instead, if you use SIFT\n",
    "\n",
    "        points1 = np.asarray(points1)\n",
    "        points1 = points1.T\n",
    "        points2 = np.asarray(points2)\n",
    "        points2 = points2.T\n",
    "        \n",
    "        th = 100\n",
    "        H_12, indices_inlier_matches_12 = Ransac_DLT_homography_adaptive_loop(points1, points2, th)\n",
    "        inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(matches_12) # matches_12 instead, if you use SIFT\n",
    "        \n",
    "        img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        plt.imshow(img_12)\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        plt.show()\n",
    "    \n",
    "#         #BUILD THE MOSAIC\n",
    "#         print(\"BUILD THE MOSAIC\")\n",
    "#         corners = [-400, 1200, -100, 650]\n",
    "        \n",
    "#         img1c = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "#         img2c = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         img1c_w = apply_H_fixed_image_size(img1c, H_12, corners)\n",
    "#         img2c_w = apply_H_fixed_image_size(img2c, np.identity(3), corners)\n",
    "#         img_mosaic_12 = np.maximum(img1c_w,img2c_w)\n",
    "        \n",
    "#         plt.imshow(img_mosaic_12)\n",
    "#         fig = matplotlib.pyplot.gcf()\n",
    "#         fig.set_size_inches(18.5, 10.5)\n",
    "#         plt.show()\n",
    "\n",
    "path_images = 'Data'\n",
    "panorama(path_images)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15** Construct the panorana image of each folder present in the Data folder and explain why it works or not in each case. In the scenarios that it does not work properly, suggest how you would try to improve the result obtained.\n",
    "\n",
    "Note 1 : Use the function created above if the case, otherwise repeate the code for each pair of images.\n",
    "\n",
    "Note 2: You will need to define new corner for panoram as the images bigger and positioned different, trick look the dimension of the images to thefine the corners of each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
