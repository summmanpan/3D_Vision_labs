{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2. Robust homography estimation: building image mosaics\n",
    "\n",
    "In this practical session, we will create panoramic images by stitching multiple images of a scene taken from the same position, but pointing the camera to slightly different directions.  These images are related by homographies that we will need to compute.  The process will have the following steps:\n",
    "\n",
    "- The first step will be to compute keypoints in the images that we can use to find correspondences.  We will use SIFT or ORB keypoints and features.\n",
    "- Next we will compute correspondences between sets of keypoint features. We will use existing code for these first two steps.\n",
    "- From the correspondences we will compute the homography that maps one image into the another. Since some correspondences may be erroneous, the computation will have to be robust to outliers. We will use the \"RANdom SAmple Consensus\" (RANSAC) method that you will have to complete.\n",
    "- Finally, with the homographies computed, we will map all the images into a common canvas by using a variant of the  `apply_H` function from the last assignment.\n",
    "\n",
    "Finally, you will have to answer the questions and complete the provided code when necessary as required. Some questions are rather theoretical and eventually imply some calculations while others are practical questions. **You must deliver the completed (and executed) ipynb file and a pdf file with the answers to the questions (included the parts of the code that need to be added).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from utils import apply_H_fixed_image_size, Normalization, DLT_homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Compute image correspondences**\n",
    "\n",
    "The first step is to read the images and to compute their keypoints. The images are RGB. We have to convert them to gray scale with values in order to compute the keypoints on them. Then, we compute the keypoints and descriptors of every image. For the parts where the content of the two images coincide, you can visually check that many of the detected points are detected in both images. We want to find these _correspondences_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img1 = cv2.imread('Data/Llanes/llanes_a.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/Llanes/llanes_b.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img3 = cv2.imread('Data/Llanes/llanes_c.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create(3000)\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "plt.imshow(img1b), plt.show()\n",
    "\n",
    "img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "plt.imshow(img2b), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Are the keypoints uniformly distributed over the image? Where are there more keypoints? Why?\n",
    "\n",
    "\n",
    "To match the keypoints between two images, we need to assign to each keypoint in the first image the one that has the most similar descriptor in the second image. \n",
    "\n",
    "Execute the following code to find image correspondences using ORB [1] (you may also use SIFT [2], SURF [3], etc, an example of the use of SIFT is given in another code cell below).\n",
    "\n",
    "[1] Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary R. Bradski. ORB: An efficient alternative to SIFT or SURF. ICCV, 2564-2571, 2011.\n",
    "\n",
    "[2] David Lowe. Object recognition from local scale-invariant features. ICCV, 1150-1157, 1999.\n",
    "\n",
    "[3] Herbert Bay, Tinne Tuytelaars, Luc Van Gool. Surf: Speeded up robust features. ECCV, 404-417, 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher()\n",
    "matches_12 = bf.knnMatch(des1,des2,k=2)\n",
    "# Apply ratio test\n",
    "good_matches_12 = []\n",
    "for m,n in matches_12:\n",
    "    if m.distance < 0.85*n.distance:\n",
    "        good_matches_12.append([m])\n",
    "\n",
    "# Show \"good\" matches \n",
    "img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to find SIFT correspondences you may use the code below. \n",
    "\n",
    "NOTE: SIFT is not available in certain OpenCV versions. A version that worked for us was opencv-contrib 4.4.0.46, you can install it with the following command: `pip install opencv-contrib-python==4.4.0.46`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create(3000)\n",
    "\n",
    "# find the keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "plt.imshow(img1b), plt.show()\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches_12 = bf.match(des1,des2)\n",
    "\n",
    "# Show matches\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.**\n",
    "Compute and visualize the matchings between image 2 and 3. Write the commands you used for that.\n",
    "\n",
    "**Q3.**\n",
    "Are all the matchings correct? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correspondences between image 2 and 3\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Compute the homography (robust DLT algorithm) between image pairs**\n",
    "\n",
    "We want now to compute the homography that relates each pair of images. From  the last assignment, we have a function called `DLT_homography` that computes a homography given a set of correspondences. Unfortunately, this only works when all of the correspondences are correct, which is not the case in most practical applictions as the current one. This time, we will need to use the RANSAC method in order to find the correct correspondences and discard the others.\n",
    "\n",
    "For that we use the functions `Ransac_DLT_homography` and `Inliers` that you have \n",
    "to complete below (this is part of questions Q5 and Q6 below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inliers(H, points1, points2, th):\n",
    "   \n",
    "    # Check that H is invertible\n",
    "    if abs(math.log(np.linalg.cond(H))) > 15: \n",
    "        idx = np.empty(1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "    # complete this code .......\n",
    "    \n",
    "    return inliers_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ransac_DLT_homography(points1, points2, th, max_it):\n",
    "    \n",
    "    Ncoords, Npts = points1.shape\n",
    "    \n",
    "    it = 0\n",
    "    best_inliers = np.empty(1)\n",
    "\n",
    "    while it < max_it:\n",
    "        L = # complete ...\n",
    "        indices = random.sample(range(1, Npts), L)\n",
    "        H = DLT_homography(points1[:,indices], points2[:,indices])\n",
    "        inliers = Inliers(H, points1, points2, th)\n",
    "        \n",
    "        # test if it is the best model so far\n",
    "        if inliers.shape[0] > best_inliers.shape[0]:\n",
    "            best_inliers = inliers\n",
    "        \n",
    "        it += 1\n",
    "    \n",
    "    # compute H from all the inliers\n",
    "    H = DLT_homography(points1[:,best_inliers], points2[:,best_inliers])\n",
    "    inliers = best_inliers\n",
    "    \n",
    "    return H, inliers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows to robustly estimate the homography that relates images 1 and 2. Examine the code and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homography between images 1 and 2\n",
    "points1 = []\n",
    "points2 = []\n",
    "for m in good_matches_12: # matches_12 instead, if you use SIFT\n",
    "    points1.append([kp1[m[0].queryIdx].pt[0], kp1[m[0].queryIdx].pt[1], 1]) # m.queryIdx instead, if you use SIFT\n",
    "    points2.append([kp2[m[0].trainIdx].pt[0], kp2[m[0].trainIdx].pt[1], 1]) # m.trainIdx instead, if you use SIFT\n",
    "    \n",
    "points1 = np.asarray(points1)\n",
    "points1 = points1.T\n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "\n",
    "th = # complete ...\n",
    "H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1, points2, th, 1000)\n",
    "inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(good_matches_12) # matches_12 instead, if you use SIFT\n",
    "\n",
    "# drawMatches instead of drawMatchesKnn if you use SIFT\n",
    "img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** What are the variables `points1` and `points2`?\n",
    "\n",
    "Complete the `Ransac_DLT_homography` function by answering the following questions:\n",
    "\n",
    "**Q5.** Set the number of samples to choose randomly and that will define the model to test in each trial. (second input parameter of function `random.sample`).\n",
    "\n",
    "**Q6.** Complete the function `Inliers` by computing the geometric error of the correspondences given the homography.\n",
    "\n",
    "**Q7.** What is the input parameter `th` when calling the `Ransac_DLT_homography` function? Choose a good value for this parameter and justify your answer.\n",
    "\n",
    "**Q8.** Create a new function `Ransac_DLT_homography_adaptive_loop` which is based on the function `Ransac_DLT_homography` and automatically adapts the number of trials to ensure we pick, with a probability $p=0.99$ an initial data set with no outliers.\n",
    "\n",
    "**Q9.** Compare experimentally the two versions of the RANSAC algorithm (`Ransac_DLT_homography` and `Ransac_DLT_homography_adaptive_loop`) in terms of the number of iterations. Which version is better and why?\n",
    "\n",
    "**Q10.** Compute the homography that relates images 2 and 3. Write the commands you used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to complete here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Build the mosaic**\n",
    "\n",
    "At this point we have all the ingredients to built the image mosaic. For transforming an image with a specified homography we use a modification of the function `apply_H` used in the previous practical session. The modified function `apply_H_fixed_image_size` transforms the input image according to the input homography and writes it in an output image of size corresponding to the input vector of desired corner coordinates.\n",
    "\n",
    "Examine and complete the code below when necessary.\n",
    "\n",
    "**Q11.** Which homography we use for transforming the image 2? Why?\n",
    "\n",
    "**Q12.** How are the two images combined in the mosaic image? Why?\n",
    "\n",
    "**Q13.** Complete the mosaic including the third image. Write the commands you used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = [-400, 1200, -100, 650]\n",
    "\n",
    "#print(img1.shape)\n",
    "img1c = cv2.imread('Data/Llanes/llanes_a.jpg',cv2.IMREAD_COLOR)\n",
    "img2c = cv2.imread('Data/Llanes/llanes_b.jpg',cv2.IMREAD_COLOR)\n",
    "img3c = cv2.imread('Data/Llanes/llanes_c.jpg',cv2.IMREAD_COLOR)\n",
    "img1c = cv2.cvtColor(img1c, cv2.COLOR_BGR2RGB)\n",
    "img2c = cv2.cvtColor(img2c, cv2.COLOR_BGR2RGB)\n",
    "img3c = cv2.cvtColor(img3c, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# mosaic 1-2\n",
    "img1c_w = apply_H_fixed_image_size(img1c, H_12, corners)\n",
    "img2c_w = apply_H_fixed_image_size(img2c, np.identity(3), corners)\n",
    "img_mosaic_12 = np.maximum(img1c_w,img2c_w)\n",
    "plt.imshow(img_mosaic_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "# mosaic 1-2-3\n",
    "# ... complete ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14.** **(Optional)** Create a function that takes as input the path to the folder where the images to construct the panorama are located. The function should work independently of the number of images that are located in the folder (assume that the images are given in the correct order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P = panorama(path_images):\n",
    "    # INPUTS:\n",
    "        #path_images: path to the folder that contain the images\n",
    "    # OTPUT: \n",
    "        # P: panoarama image \n",
    "        \n",
    "    \n",
    "    # READ ALL THE IMAGES OF THE INPUT PATH  \n",
    "    \n",
    "    # COMPUTE THE IMAGE KEYPOINTS OF THE IMAGES\n",
    "    \n",
    "    # MATCH THE KEYPOINTS\n",
    "    \n",
    "    #COMPUTE THE HOMOGRAPIES\n",
    "    \n",
    "    #BUILD THE MOSAIC\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15** Construct the panorana image of each folder present in the Data folder and explain why it works or not in each case. In the scenarios that it does not work properly, suggest how you would try to improve the result obtained.\n",
    "\n",
    "Note 1 : Use the function created above if the case, otherwise repeate the code for each pair of images.\n",
    "\n",
    "Note 2: You will need to define new corner for panoram as the images bigger and positioned different, trick look the dimension of the images to thefine the corners of each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
