{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Camera calibration using a planar pattern\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this assignment we are going to find the camera calibration parameters using a planar pattern.  The method consists of taking several images of a known planar pattern in different positions.  By detecting the pattern in the images, we will be able to recover the camera internal parameters as well as its pose relative to the pattern.  This will enable us to draw the cameras in 3D and to project 3D objects onto the images as in Augmented Reality applications.\n",
    "\n",
    "### 1.1 The pattern\n",
    "The planar pattern can be any flat image, usually a chessboard.  For simplicity, instead of the chessboard, we are going to use a flat screen to display a texture-rich image.  The ORB/SIFT/SURF descriptor will be used to compute correspondences between the picture of the screen with the picture stored in the computer, as it has been done in previous assignments for computing homographies.\n",
    "\n",
    "### 1.2 Taking the images\n",
    "You will use the provided images in a first stage.  Later on, you are asked to take pictures of a flat pattern with any other camera you want to calibrate.\n",
    "\n",
    "### 1.3 Goals and Requirements\n",
    "\n",
    "The goal of the current lab assignment is to learn the following concepts:\n",
    "\n",
    "- How to compute the internal camera parameters from a set of plane homographies.  We will use the Image of the Absolute Conic for this. \n",
    "\n",
    "- How to compute the camera position with respect to the calibration target.\n",
    "\n",
    "- How to add simple virtual 3D objects on the images of the calibration target.\n",
    "\n",
    "\n",
    "We will assume that we already know the following concepts:\n",
    "\n",
    "- What is a homography, how to match ORB/SIFT/SURF keypoints between images and how to robustly compute a homography from point correspondences.\n",
    "\n",
    "- What is the camera projection matrix and what are the internal and external camera parameters and their matrix form $P = K(R|t)$.\n",
    "\n",
    "- You have an idea of what is the Image of the Absolute Conic and you know that its matrix is simply $w = (K K ^T)^{-1} = K^{-T} K^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from utils import Ransac_DLT_homography, plot_camera, plot_image_origin\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibration with a planar pattern\n",
    "\n",
    "### 2.1 Load the images and compute homographies\n",
    "\n",
    "As in the previous session, we are going to compute homographies between images using ORB/SIFT/SURF matching and RANSAC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = cv2.imread('Data/template.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "images = []\n",
    "N = 3 # 5\n",
    "for i in range(1,N+1):\n",
    "    m = cv2.imread(\"Data/calib{0}.jpg\".format(i),cv2.IMREAD_GRAYSCALE)\n",
    "    images.append(m)\n",
    "    \n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create(4000)\n",
    "\n",
    "# find the keypoints and descriptors\n",
    "kpt, dest = sift.detectAndCompute(template,None)\n",
    "kpi, desi = [], []\n",
    "for m in images:\n",
    "    kp, des = sift.detectAndCompute(m,None)\n",
    "    kpi.append(kp)\n",
    "    desi.append(des)\n",
    "\n",
    "\n",
    "# Keypoint matching and homography estimation\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "H = []\n",
    "for i in range(N):\n",
    "    matches = bf.match(dest,desi[i])\n",
    "    \n",
    "    # Fit homography and remove outliers\n",
    "    points1 = []\n",
    "    points2 = []\n",
    "    for m in matches:\n",
    "        points1.append([kpt[m.queryIdx].pt[0], kpt[m.queryIdx].pt[1], 1])\n",
    "        points2.append([kpi[i][m.trainIdx].pt[0], kpi[i][m.trainIdx].pt[1], 1])\n",
    "        \n",
    "    points1 = np.asarray(points1)\n",
    "    points1 = points1.T\n",
    "    points2 = np.asarray(points2)\n",
    "    points2 = points2.T\n",
    "    \n",
    "    Hi, indices_inlier_matches = Ransac_DLT_homography(points1, points2, 3, 1000)\n",
    "    H.append(Hi)\n",
    "    \n",
    "    # Show inlier matches\n",
    "    inlier_matches = itemgetter(*indices_inlier_matches)(matches)\n",
    "    img_12 = cv2.drawMatches(template,kpt,images[i],kpi[i],inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compute the Image of the Absolute Conic\n",
    "\n",
    "Given the homographies we are now going to compute the Image of the Absolute conic (IAC).  Read carefully the theory in the provided file 'calibration.pdf'.\n",
    "\n",
    "**Q1.** Complete the code below to compute the IAC from the homographies.\n",
    "\n",
    "**Q2.** How many images are required to compute the IAC? What would you do if you had fewer images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Image of the Absolute Conic\n",
    "A = np.zeros((2*N, 6))\n",
    "\n",
    "for i in range(N):\n",
    "    h = H[i]\n",
    "    A[2*i-1, :] = np.array([h[0,0]*h[0,1], h[0,0]*h[1,1] + h[1,0]*h[0,1], h[0,0]*h[2,1] + h[2,0]*h[0,1], h[1,0]*h[1,1], h[1,0]*h[2,1] + h[2,0]*h[1,1], h[2,0]*h[2,1] ])\n",
    "    A[2*i, :] = np.array([h[0,0]**2, 2*h[0,0]*h[1,0], 2*h[0,0]*h[2,0], h[1,0]**2, 2*h[1,0]*h[2,0], h[2,0]**2])\n",
    "    A[2*i, :] -= np.array([h[0,1]**2, 2*h[0,1]*h[1,1] , 2*h[0,1]*h[2,1], h[1,1]**2, 2*h[1,1]*h[2,1], h[2,1]**2])\n",
    "\n",
    "# complete ... \n",
    "w = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Compute the camera calibration matrix from the IAC\n",
    "\n",
    "The IAC relates to the camera calibration matrix, $K$ as $\\omega = K^{-T} K^{-1}$. Knowing $\\omega$ we can get $K$ using the Cholesky factorization.\n",
    "\n",
    "**Q3.** Write the code to compute the camera calibration from the IAC.\n",
    "\n",
    "**Q4.** What result do you get? Can you interpret the values of $K$ in geometric terms? Which ones are related to the image size? How are they related? Do they make sense in your results?\n",
    "\n",
    "**Q5.** Compute the field of view angle of the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete ... \n",
    "K = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compute camera position and orientation\n",
    "\n",
    "Given the computed calibration $K$ and the homografies $H$, we can compute the 3D position and orientation of the camera with respect to the planar pattern.\n",
    "\n",
    "**Q6.** Complete the code to compute the rotation and translation of each camera.\n",
    "\n",
    "**Q7.** In which units are expressed the translation vector `ti`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute camera position and orientation\n",
    "import math\n",
    "R = []\n",
    "t = []\n",
    "P = []\n",
    "\n",
    "for i in range(N):\n",
    "    # compute r1, r2, and t{i}\n",
    "    h = H[i]\n",
    "    r1 = # complete ...\n",
    "    r2 = # complete ...\n",
    "    ti = # complete ...\n",
    "    \n",
    "    # Solve the scale ambiguity by forcing r1 and r2 to be unit vectors.\n",
    "    s = math.sqrt(np.linalg.norm(r1) * np.linalg.norm(r2)) * np.sign(ti[2])\n",
    "    r1 = r1 / s\n",
    "    r2 = r2 / s\n",
    "    ti = ti / s\n",
    "    t.append(ti)\n",
    "    Ri = np.array([r1, r2, np.cross(r1,r2)])\n",
    "    Ri = Ri.T\n",
    "    \n",
    "    # Ensure R is a rotation matrix\n",
    "    #[U S V] = svd(R{i});\n",
    "    U, d, Vt = np.linalg.svd(Ri)\n",
    "    Ri = U @ np.identity(3) @ Vt\n",
    "    R.append(Ri)\n",
    "   \n",
    "    # Pi = K * [Ri ti]\n",
    "    A = np.zeros((3,4))\n",
    "    A[:3,:3] = Ri\n",
    "    A[:,3] = ti\n",
    "    Pi = K @ A\n",
    "    P.append(Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows to visualize in 3D the different cameras corresponding to the different images and the frame (rectangular boundaries) of the planar pattern. The cameras are illustrated with a pyramid that encodes the estimated position (apex) and orientation (base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go    \n",
    "\n",
    "ny, nx = images[0].shape\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(N):\n",
    "    plot_camera(P[i], nx, ny, fig, \"camera{0}\".format(i))\n",
    "\n",
    "plot_image_origin(nx/2, ny/2, fig, \"image\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Complete the code below to plot a moving planar target in front of a static camera.\n",
    "\n",
    "**Q9.** When taking the images, we did move the camera and the planar pattern was static. The last plot shows a moving pattern in front of a static camera.  What is the relation between the two situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_Rt(R,t,w, h, fig, legend):\n",
    "    \n",
    "    p1 = # to complete ...\n",
    "    p2 = # to complete ...\n",
    "    p3 = # to complete ...\n",
    "    p4 = # to complete ...\n",
    "    \n",
    "    x = np.array([p1[0], p2[0], p3[0], p4[0], p1[0]])\n",
    "    y = np.array([p1[1], p2[1], p3[1], p4[1], p1[1]])\n",
    "    z = np.array([p1[2], p2[2], p3[2], p4[2], p1[2]])\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(x=x, y=z, z=-y, mode='lines',name=legend))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "A = np.zeros((3,4))\n",
    "A[0,0]=A[1,1]=A[2,2]=1\n",
    "\n",
    "plot_camera(K@A, nx, ny, fig, \"camera\")\n",
    "for i in range(N):\n",
    "    plot_image_Rt(R[i], t[i], nx/2, ny/2, fig, \"image{0}\".format(i))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Augmented reality\n",
    "\n",
    "We now have the projection matrices, $P$, of each image with respect to the planar target's reference frame. We can use these projection matrices for projecting 3D points onto the images. Given a 3D point, we can project it using the projection matrix and plot the projection over the image.\n",
    "\n",
    "**Q10.** Complete the code below to plot a cube in front of the calibration pattern on each image. The result should look as if a cube was attached to the pattern as the pattern moves from image to image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Th, Tw = template.shape\n",
    "cube_corners = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1], [0, 0, 1], [0, 0, 0], [1, 0, 0], [1, 0, 1], [0, 0, 1], [0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 1, 1], [0, 1, 1], [0, 1, 0]])\n",
    "\n",
    "import numpy.matlib\n",
    "offset = np.array([Tw/2, Th/2, -Tw/8])\n",
    "M = cube_corners.shape[0]\n",
    "X = (cube_corners - 0.5) * Tw/ 4 + np.matlib.repmat(offset, M, 1)\n",
    "\n",
    "X = X.T\n",
    "ones = np.ones(M)\n",
    "Xh = np.stack((X[0,:], X[1,:], X[2,:], ones), axis=0)\n",
    "\n",
    "line_color = (0, 255, 0)\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    xp = # complete ...\n",
    "    \n",
    "    image = cv2.imread(\"Data/calib{0}.jpg\".format(i+1),cv2.IMREAD_COLOR) \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_gray = image\n",
    "    image_gray[:, :, 0] = gray[:, :]\n",
    "    image_gray[:, :, 1] = image_gray[:, :, 0]\n",
    "    image_gray[:, :, 2] = image_gray[:, :, 0]\n",
    "    for j in range(M-1):\n",
    "        image = cv2.line(image_gray, (xp[0,j],xp[1,j]), (xp[0,j+1],xp[1,j+1]), line_color, 4) \n",
    "    plt.imshow(image)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Repeat all the process using your own images (you may use from 3 to 5 different views of a planar pattern). Comment the results and try to find an explanation in case it doesn't work.\n",
    "\n",
    "### 2.6 Reducing the number of images\n",
    "\n",
    "In case we have some knowledge about the camera parameters like the principal point, the aspect ratio or zero skew factor the number of images needed for calibration can be reduced.\n",
    "\n",
    "**Q12.** Add a linear constraint to enforce skew 0.  See Zhang's paper (zhang98.pdf) [1]. What is the minimum number of views you need to calibrate the camera in this case?\n",
    "\n",
    "**Q13.** Can you calibrate from a single image? How? Adapt your code to be able to calibrate from just a single image. Compare the results with the ones obtained when using more images. \n",
    "\n",
    "**Optional:** Using the last result you can create a video where the projected virtual cube automatically adapts to the changing point of view when you move the camera.\n",
    "\n",
    "[1] Zhengyou Zhang. A flexible new technique for camera calibration, IEEE Transactions on pattern analysis and machine intelligence, 22(11), 1330-1334, 2000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
