{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> <font size=\"3\"> Suman Pan - 218352 </mark>\n",
    "\n",
    "\n",
    "<mark> <font size=\"3\"> Clara Ll√≤ria - 218147 </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Stereo Matching and Shape from Silhouette\n",
    "\n",
    "This lab session is divided in two parts. In the first one, you will compute a disparity map from a pair of images.  A disparity map is an image that stores the inverse of the distance to the camera center of the points\n",
    "seen at every pixel, i.e. the depth.  This will enable us, for example in case our cameras are calibrated, to display a dense cloud of points representing the scene.\n",
    "\n",
    "To compute the disparity map with a local method, we will have to compute correspondences for every pixel in the reference image.  These are many more correspondences than the ones given by SIFT/SURF/ORB.  For every pixel in the first image, we will travel its corresponding epipolar line in the second image looking for its correspondence. Since the pair of images are stereo-rectified (parallel views) the epipolar lines are horizontal and the coordinates of the corresponding points differ just by a horizontal displacement, the disparity.  By comparing the information around a neighborhood of the pixels on both images we will decide whether the pixels do correspond to each other.\n",
    "\n",
    "In the second part, you will compute a 3D reconstruction of an object given some binary images corresponding to different points of view. These binary images, called the silhouettes, contain a segmentation of the object of interest that we want to reconstruct.\n",
    "\n",
    "In particular, the goal of this lab session is to learn the following concepts:\n",
    "\n",
    "- How to compute disparity/depth maps from pairs of images.\n",
    "- Compare different cost functions and window sizes.\n",
    "- How to aggregate costs through bilateral weights.\n",
    "- How to extract a Visual Hull from a set of different views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stereo matching\n",
    "\n",
    "First we will work with a pair of stereo-recified images from the Middlebury stereo dataset: http://vision.middlebury.edu/stereo/. The ground truth disparity is available and it will be useful to validate the code to be completed and it will allow us to measure errors as well.\n",
    "\n",
    "The incomple function `stereo_computation` is provided; you have to complete it with the SSD cost, the NCC similarity measure and the bilateral weights. \n",
    "\n",
    "**Q1.** Complete the `stereo_computation` function with the computation of the SSD cost.\n",
    "\n",
    "**Q2.** Execute cell code given after the `stereo_computation` function. This code estimates the disparity between a pair of stereo rectified images. How do the error in the estimation and the occluded areas are related?\n",
    "\n",
    "**Q3.** Compute the global error.\n",
    "\n",
    "**Q4.** Evaluate the results changing the window size (e.g. 5$\\times$5, 9$\\times$9, 21$\\times$21). Comment the results and the differences among the different results.\n",
    "\n",
    "**Q5.** Complete the `stereo_computation` function with the computation of the NCC cost.\n",
    "\n",
    "**Q6.** Compare the results with those obtained with the SDD cost.\n",
    "\n",
    "**Q7.** Complete the `stereo_computation` function with the bilateral weights. Suggested parameters: $\\gamma_{col}=12$ and `math.floor(win_size/2)` for $\\gamma_{pos}$.\n",
    "\n",
    "**Q8.** Evaluate the results changing the window size (e.g. 5 $\\times$ 5, 9 $\\times$ 9, 21 $\\times$ 21) and compare to the previous case that uses uniform weights (SDD cost).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_computation(color_left_img, color_right_img, min_disparity, max_disparity, win_size, bilateral_weights=False, method='ssd'):\n",
    "    \"\"\"\n",
    "    -left image\n",
    "    -right image\n",
    "    -minimum disparity\n",
    "    -maximum disparity\n",
    "    -window size\n",
    "    -bilateral_weights\n",
    "    -matching cost\n",
    "    \"\"\"\n",
    "    \n",
    "    # conversion to float images to make calculations\n",
    "    color_left_img = color_left_img.astype(np.float32)\n",
    "    color_right_img = color_right_img.astype(np.float32)\n",
    "    \n",
    "    # complete ...\n",
    "\n",
    "    # iterate over the image\n",
    "    for i in range(win_step, height-win_step):\n",
    "        for j in range(win_step, width-win_step):\n",
    "            \n",
    "            # complete ...\n",
    "\n",
    "            if method=='ssd': # sum of squared diff pag25 T8\n",
    "                \n",
    "                # complete ...\n",
    "\n",
    "            elif method=='ncc': \n",
    "                # PAG 25 TEORIA 8\n",
    "                # complete ...\n",
    "                \n",
    "            else:\n",
    "                raise(NameError)\n",
    "            \n",
    "\n",
    "    return disparity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data images (rectified images)\n",
    "img1 = cv2.imread('./data1/scene1.row3.col3.ppm')\n",
    "img2 = cv2.imread('./data1/scene1.row3.col2.ppm')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.subplot(1,2,1),plt.imshow(img1)\n",
    "plt.title('Left image')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(1,2,2),plt.imshow(img2)\n",
    "plt.title('Right image')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Disparity ground truth image\n",
    "img_gt = cv2.imread('./data1/truedisp.row3.col3.pgm', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img_gt,'gray')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.show()\n",
    "\n",
    "# Set minimum and maximum disparity\n",
    "min_disparity = 0\n",
    "max_disparity = 16\n",
    "\n",
    "# Set window (patch) size\n",
    "w_size = 9\n",
    "ssd_disp = np.zeros(shape=(img1.shape[0], img1.shape[1]))\n",
    "ssd_disp = stereo_computation(img1, img2, min_disparity, max_disparity, w_size, bilateral_weights=False, method='ssd')\n",
    "\n",
    "# Display the error image with respect to the ground truth\n",
    "plt.imshow(ssd_disp,'gray')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.title(\"Estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code is completed and you finish the experiments with the Middlebury image, the next step is to apply the code to our lab 4 images. These images are not rectified, we have rectified them by applying a proper homography to each one of them. The stereo-rectified images are the ones provided in this lab. The lab images are big. To speed up the computations, you can scale-down the images by a factor of 0.25. Once everything is working properly, you can increase the scale in order to get higher resolution results.\n",
    "\n",
    "**Q9.** Set proper values for the disparity limits.\n",
    "\n",
    "**Q10.** Comment the result obtained. Why do you think the disparity is not well estimated\n",
    "in some parts of the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data images (rectified images)\n",
    "img1 = cv2.imread('./data1/rectif0.png')\n",
    "img2 = cv2.imread('./data1/rectif1.png')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# complete ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shape from silhouette: visual hull method\n",
    "\n",
    "The Visual Hull is a 3D reconstruction technique that allows to recover a 3D shape given its silhouettes in different views. A silhouette is a mask that provides a segmentation of the foreground object. The Visual Hull may be obtained as the intersection of the visual cones of the different points of view, which are generated by backprojection of the silhouttes using camera parameters. The advantage of this method is that it is very simple, it is non-iterative and can be easily parallelized. Its main disadvantage is the inability to obtain the exact shape of the object (e.g. cavities in the object cannot be recovered) but it is widely used with multi-view stereo algorithms, either as a first step or in conjunction with the multi-view reconstruction providing additional constraints.\n",
    "In practice, the visual cones are not estimated, instead, a voxel grid is defined and all the voxels of the grid are projected onto each image ir order to check if the voxel projects inside the object silhouette or not.\n",
    "\n",
    "In order to visualize the 3D volume we will use the Python library IPyvolume: https://ipyvolume.readthedocs.io/en/latest/ \n",
    "\n",
    "You will have to install the library, more information here: https://ipyvolume.readthedocs.io/en/latest/install.html\n",
    "\n",
    "In our case it worked with the following commands:\n",
    "\n",
    "- `pip install ipyvolume`\n",
    " \n",
    "- `jupyter nbextension enable --py --sys-prefix ipyvolume` \n",
    " \n",
    "- `jupyter nbextension enable --py --sys-prefix widgetsnbextension`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "\n",
    "# Shape from silhouette - Visual Hull algorithm\n",
    "\n",
    "# Define variables\n",
    "bbox = np.array([[0.2, -0.3, -1.8],[2.2, 1.3, 2.7]]) # [minX minY minZ; maxX maxY maxZ]\n",
    "volumeX = 40 # 64 # start with a small volume and once code works properly you can increase its size\n",
    "volumeY = 40 # 64\n",
    "volumeZ = 80 # 128\n",
    "volumeThreshold = 17\n",
    "silhouetteThreshold = 100\n",
    "numCameras = 18\n",
    "\n",
    "\n",
    "# Load silhouette images and projection matrices\n",
    "images = []\n",
    "Ps = []\n",
    "for i in range(numCameras):\n",
    "    \n",
    "    P = np.loadtxt(\"data2/david_{:02d}.pa\".format(i), delimiter=' ')\n",
    "    Ps.append(P)\n",
    "    \n",
    "    m = cv2.imread(\"data2/david_{:02d}.jpg\".format(i),cv2.IMREAD_GRAYSCALE)\n",
    "    ret,sil = cv2.threshold(m,silhouetteThreshold,255,cv2.THRESH_BINARY)\n",
    "    images.append(sil)\n",
    "    \n",
    "    plt.subplot(1,2,1),plt.imshow(m,'gray',vmin=0,vmax=255)\n",
    "    plt.title('image')\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "    plt.subplot(1,2,2),plt.imshow(sil,'gray',vmin=0,vmax=255)\n",
    "    plt.title('silhouette')\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "T = np.identity(4)\n",
    "T[:3,3] = bbox[0,:]\n",
    "T = T @ np.diag([(bbox[1,0]-bbox[0,0])/volumeX, (bbox[1,1]-bbox[0,1])/volumeY, (bbox[1,2]-bbox[0,2])/volumeZ, 1])\n",
    "\n",
    "F = np.array([[1, 0, 0, 0],[0, 0, 1, 0],[0, -1, 0, 0],[0, 0, 0, 1]]) # flip y and z axes for better display in isosurface\n",
    "T = F @ T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell above.\n",
    "\n",
    "**Q11.** How are the silhouettes extracted in the provided code? Would it work in a general case? Why?\n",
    "\n",
    "Execute the following cell code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Draw projection of volume corners\n",
    "for i in range(numCameras):\n",
    "\n",
    "    corners = np.array([[0, 0, 0, 1],[0, 0, volumeZ, 1],[0, volumeY, 0, 1], [0, volumeY, volumeZ, 1], [volumeX, 0, 0, 1], [volumeX, 0, volumeZ, 1], [volumeX, volumeY, 0, 1], [volumeX, volumeY, volumeZ, 1]])\n",
    "    corners = corners.T\n",
    "    pcorners = Ps[i] @ T @ corners\n",
    "    pcorners = pcorners / pcorners[2][np.newaxis]\n",
    "    \n",
    "    img_path = \"data2/david_{:02d}.jpg\".format(i)\n",
    "    I = Image.open(img_path)\n",
    "    plt.figure()\n",
    "    canv2 = ImageDraw.Draw(I)\n",
    "    for i in range(8):\n",
    "        canv2.ellipse((pcorners[0,i], pcorners[1,i], pcorners[0,i]+7, pcorners[1,i]+7), fill = 'yellow', outline ='yellow')\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "    plt.imshow(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** In your opinion, what is the projection of volume corners useful for.\n",
    "\n",
    "The following cell contains the visual hull algorithm with a missing part that needs to be completed.\n",
    "\n",
    "**Q13.** Complete the missing part of the code and execute the final cell to visualize the 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Hull computation\n",
    "\n",
    "# PAGS 11-13 TEORIA 9\n",
    "volume = np.zeros((volumeX,volumeY,volumeZ))\n",
    "\n",
    "x = np.arange(0.5, volumeX, 1)\n",
    "y = np.arange(0.5, volumeY, 1)\n",
    "z = np.arange(0.5, volumeZ, 1)\n",
    "voxel3Dx, voxel3Dy, voxel3Dz = np.meshgrid(x, y, z)\n",
    "\n",
    "ny, nx = images[0].shape\n",
    "    \n",
    "for n in range(numCameras):\n",
    "    for x in range(volumeX):\n",
    "        for y in range(volumeY):\n",
    "            for z in range(volumeZ):\n",
    "                voxel = np.array([voxel3Dx[x,y,z], voxel3Dy[x,y,z], voxel3Dz[x,y,z], 1])\n",
    "                world_coords = T @ voxel.T\n",
    "                # complete ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv.figure()\n",
    "ipv.plot_isosurface(volume, level=volumeThreshold, color='gray', extent=[[0.2, 2.2], [-0.3, 1.3], [-1.8, 2.7]])\n",
    "ipv.squarelim()\n",
    "ipv.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
